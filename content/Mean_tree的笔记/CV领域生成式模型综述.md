# CV领域生成式模型原理简要总结
## 0. 
近年的AI研究都在向AIGC、多模态模型转变，笔者的导师也说明实验室近期的研究重点是AIGC方向。而笔者本人的数学基础较差，而今后的学习将不可避免的要接触生成式模型，因此这里简要对CV领域的生成式模型进行知识梳理。

这里因为笔者的latex公式编辑能力非常孱弱，因此利用LLM进行辅助（使用GAI写GAI的笔记合情合理）
如果想跳过任何数学知识快速入门，推荐一篇由MSRA研究员撰写的[知乎文章](https://zhuanlan.zhihu.com/p/591881660)，同时知乎还有很多介绍生成式模型的文章均被参考，这里不再详细引用。

## 1.GAN(Generative Adversarial Networks,生成对抗网络)
[论文链接](https://arxiv.org/abs/1406.2661)
GAN，即生成对抗网络，在2014年发表，一度大热，而diffusion系列诞生后迅速过时，但是作为CV领域的生成式模型的总结，还是需要包括其内容。

### 模型组成：
#### 生成器（Generator）
- 目标：产生逼真的数据
- 输入：随机噪声$z$，通常来自高斯分布
- 输出：生成的数据$G(z)$

#### 判别器（Discriminator）
- 目标：区分真实数据与生成数据
- 输入：真实数据$x$或生成数据$G(z)$
- 输出：数据为真的概率$D(x)$

GANs 的训练是一个动态的博弈过程，可以理解为生成器在造假生成非现实的图片骗过判别器，判别器则努力分辨哪些图片是伪造的
**数学表述：**
$$
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log (1 - D(G(z)))]
$$

这里，$p_{data}(x)$是真实数据的分布，$p_{z}(z)$是噪声的分布，$G$和$D$分别是生成器和判别器的参数。

GAN的最大问题是其训练非常困难，相当于同时训练两个模型，而且要保证两者高效率的同时最后一定是“造假者”胜出。很多改进就是在改善这个问题，其他模型则因为没有这个问题而能够取代它。
## 2.VAE(Variational Auto-Encoder, 变分自编码器)及其衍生
### 2.1 AE，autoencoder
#### 模型组成
AE是经典的Encoder-Decoder架构，从U-Net起这个架构就经久不衰。值得注意的是现在很多生成式模型中能够看到U-Net的影子，2024年大热的文生图stable-diffusion就使用了U-Net网络。这个只有5层深度的简单的卷积网络似乎有很大启发性。
##### 编码器（Encoder）
- 目标：将输入数据压缩成一个潜在空间的低维表示。
- 输入：原始数据$x$
- 输出：潜在空间的编码$z = f(x; \theta_e)$，$\theta_e$为编码器参数

##### 解码器（Decoder）
- 目标：从潜在空间的表示重构原始数据。
- 输入：编码$z$
- 输出：重构的数据$\hat{x} = g(z; \theta_d)$，$\theta_d$为解码器参数


自编码器的训练是通过最小化重构误差来完成的。网络学习如何编码和解码数据，使得重构的数据尽可能接近原始数据。这个模型主要的作用是特征提取和图片压缩、去噪，**不能生成**。其对于输入没有随机性，把数据建模为特征而不是其他生成模型的概率分布。生成则是VAE要做的工作。
### 2.2 VAE
#### 模型组成
##### 编码器
编码器负责将输入数据$x$映射到一个潜在空间的分布参数上。在数学上，编码器输出潜在变量$z$的参数，通常是均值$\mu$和方差$\sigma^2$（或更常用的对数方差$\log \sigma^2$）。

##### 解码器
解码器的任务是从潜在空间的分布中采样一个点$z$，并将其映射回数据空间，尝试重构原始数据$x$。

#### 数学原理

VAE的核心数学原理是最小化重参数化后的重构损失和KL散度的组合。重参数化技巧（reparameterization trick）允许我们通过采样的方式避免对不可微的分布参数直接进行梯度下降。

##### 重参数化技巧
这里有一个问题就是，decoder要在encoder的输出中采样$z$,而神经网络的学习过程是反向传播算法，即通过链式法则求导。显然**采样这一操作不可求导**，因此作者们天才般地提出了reparametrize(重采样)这个trick。

重参数化技巧通过以下步骤实现：
1. 编码器输出潜在变量的参数$\mu$和$\log \sigma^2$。
2. 从标准正态分布中采样一个随机噪声$\epsilon \sim \mathcal{N}(0, I)$。
3. 计算采样的潜在变量$z = \mu + \sigma \odot \epsilon$。
   
这样反向传播时，我们就可以对$\mu$和$\sigma$进行求导了。

##### 损失函数
VAE的损失函数由两部分组成：
- **重构损失**：通常使用均方误差（MSE）或二元交叉熵（BCE）来衡量重构数据与原始数据之间的差异，直白地说就是使生成的图像**像真实图像**
- **KL散度**：衡量编码器输出的潜在分布与先验分布（通常是标准正态分布）之间的差异。因为随着训练，encoder会越来越倾向产生固定的$z$,标准差趋于0，VAE变成了AE而我们的目的是使其趋于正态分布。基于此构造了KL损失，计算标准正态分布和输出之间的KL散度。

损失函数的形式如下：
$$
\mathcal{L} = -\mathbb{E}[\log p(x|z)] + KL(q(z|x) || p(z)) 
$$

$$
\text{KL}(q(z|x) \| p(z)) = \frac{1}{2} \sum_{i} \left( \log \sigma_i^2 + \mu_i^2 - \sigma_i^2 - 1 \right) 
$$
这个KL散度的推导过程见此[文章](https://www.cnblogs.com/amazingter/p/14686450.html#:~:text=AE%E4%B8%BB%E8%A6%81%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE,%E5%88%99%E8%A6%81%E7%94%A8%E5%88%B0GAN%E3%80%82),如果觉得我写的不好可以直接看这个学习VAE。



